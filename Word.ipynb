{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "dropout       = 0.5\n",
    "batch_size    = 20\n",
    "\n",
    "# Embedding\n",
    "embedding_dim = 200\n",
    "\n",
    "# RNN\n",
    "hidden_dim    = 200\n",
    "num_layers    = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from english_data_provider import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigo = EnglishDataProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x, my_y = eigo.get_word_pairs('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(eigo.get_vocabulary())\n",
    "print 'Vocabulary size:', vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = eigo.get_word_pairs('train')\n",
    "valid_x, valid_y = eigo.get_word_pairs('valid')\n",
    "test_x , test_y  = eigo.get_word_pairs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sequence length: 35\n"
     ]
    }
   ],
   "source": [
    "input_seq_length = len(train_x[0])\n",
    "print' Sequence length:', input_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.placeholder(tf.int32, shape=[batch_size, input_seq_length], name=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = tf.placeholder(tf.int64, [batch_size, input_seq_length], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.get_variable('word_embedding', [vocab_size, embedding_dim])\n",
    "input_embedded = tf.nn.embedding_lookup(embeddings, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_layers == 1:\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim, state_is_tuple=True, forget_bias=0.0)\n",
    "else:\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(hidden_dim, state_is_tuple=True, forget_bias=0.0)\n",
    "                for _ in range(num_layers)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = cell.zero_state(batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_inputs = tf.reshape(input_embedded, [batch_size, input_seq_length, -1])\n",
    "rnn_inputs = [tf.squeeze(x, [1]) for x in tf.split(rnn_inputs, input_seq_length, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, final_rnn_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_W = tf.get_variable('outW', shape=[hidden_dim, vocab_size])\n",
    "output_b = tf.get_variable('outB', shape=[vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    logits += [ tf.matmul(output, output_W) + output_b ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_35:0' shape=(20,) dtype=int64>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(tf.split(targets, input_seq_length, 1)[0], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [tf.squeeze(x, [1]) for x in tf.split(targets, input_seq_length, 1)]\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Builds training graph. '''\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr = 1.0\n",
    "max_grad_norm = 5.0\n",
    "\n",
    "with tf.variable_scope('SGD_Training'):\n",
    "    # SGD learning parameter\n",
    "    learning_rate = tf.Variable(lr, trainable=False, name='learning_rate')\n",
    "\n",
    "    # collect all trainable variables\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, global_norm = tf.clip_by_global_norm(tf.gradients(loss * input_seq_length, tvars), max_grad_norm)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)\n",
    "session.run(tf.assign(learning_rate, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size():\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    size = 0\n",
    "    for x in params:\n",
    "        sz = 1\n",
    "        for dim in x.get_shape():\n",
    "            sz *= dim.value\n",
    "        size += sz\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_char_embedding_padding = tf.scatter_update(embeddings, [0], tf.constant(0.0, shape=[1, embedding_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Created and initialized fresh model. Size:', 4651600)\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(clear_char_embedding_padding)\n",
    "print('Created and initialized fresh model. Size:', model_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' training starts here '''\n",
    "best_valid_loss = None\n",
    "rnn_state = session.run(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500: 0 [  500/ 1327], train_loss/perplexity = 5.74982834/314.1367188 secs/batch = 0.0505s, grad.norm=5.22597599\n",
      "  1000: 0 [ 1000/ 1327], train_loss/perplexity = 5.56640863/261.4932861 secs/batch = 0.0540s, grad.norm=5.82978010\n",
      "('Epoch training time:', 70.69475293159485)\n",
      "('at the end of epoch:', 0)\n",
      "train loss = 5.43245171, perplexity = 228.70928667\n",
      "validation loss = 5.43693614, perplexity = 229.73722087\n",
      "('Saved model', 'cv/w_epoch000_5.4369.model')\n",
      "  1827: 1 [  500/ 1327], train_loss/perplexity = 4.89489269/133.6056671 secs/batch = 0.0542s, grad.norm=6.75933123\n",
      "  2327: 1 [ 1000/ 1327], train_loss/perplexity = 5.13991594/170.7014160 secs/batch = 0.0547s, grad.norm=6.55329752\n",
      "('Epoch training time:', 70.20807313919067)\n",
      "('at the end of epoch:', 1)\n",
      "train loss = 5.03276399, perplexity = 153.35630210\n",
      "validation loss = 5.15711841, perplexity = 173.66330807\n",
      "('Saved model', 'cv/w_epoch001_5.1571.model')\n",
      "  3154: 2 [  500/ 1327], train_loss/perplexity = 4.60654688/100.1377640 secs/batch = 0.0540s, grad.norm=7.09215593\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    epoch_start_time = time.time()\n",
    "    avg_train_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, len(train_x) - batch_size, batch_size):\n",
    "        count += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        t_loss, _, rnn_state, gradient_norm, step, _ = session.run([\n",
    "            loss,\n",
    "            train_op,\n",
    "            final_rnn_state,\n",
    "            global_norm,\n",
    "            global_step,\n",
    "            clear_char_embedding_padding\n",
    "        ], {\n",
    "            input_ : train_x[i:i+batch_size],\n",
    "            targets: train_y[i:i+batch_size],\n",
    "            initial_state: rnn_state,\n",
    "            keep_prob: 1.0 - dropout\n",
    "        })\n",
    "\n",
    "        avg_train_loss += 0.05 * (t_loss - avg_train_loss)\n",
    "\n",
    "        time_elapsed = time.time() - start_time\n",
    "\n",
    "        if count % 500 == 0:\n",
    "            print('%6d: %d [%5d/%5d], train_loss/perplexity = %6.8f/%6.7f secs/batch = %.4fs, grad.norm=%6.8f' % (step,\n",
    "                                                    epoch, count,\n",
    "                                                    len(train_x)/batch_size,\n",
    "                                                    t_loss, np.exp(t_loss),\n",
    "                                                    time_elapsed,\n",
    "                                                    gradient_norm))\n",
    "\n",
    "    print('Epoch training time:', time.time()-epoch_start_time)\n",
    "    \n",
    "    # epoch done: time to evaluate\n",
    "    avg_valid_loss = 0.0\n",
    "    count = 0\n",
    "    rnn_state = session.run(initial_state)\n",
    "    for i in range(0, len(valid_x) - batch_size, batch_size):\n",
    "        count += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        t_loss, rnn_state = session.run([\n",
    "            loss,\n",
    "            final_rnn_state\n",
    "        ], {\n",
    "            input_ : valid_x[i:i+batch_size],\n",
    "            targets: valid_y[i:i+batch_size],\n",
    "            initial_state: rnn_state,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        \n",
    "        avg_valid_loss += t_loss / (len(valid_x)/batch_size)\n",
    "\n",
    "    print(\"at the end of epoch:\", epoch)\n",
    "    print(\"train loss = %6.8f, perplexity = %6.8f\" % (avg_train_loss, np.exp(avg_train_loss)))\n",
    "    print(\"validation loss = %6.8f, perplexity = %6.8f\" % (avg_valid_loss, np.exp(avg_valid_loss)))\n",
    "\n",
    "    save_as = '%s/w_epoch%03d_%.4f.model' % ('cv', epoch, avg_valid_loss)\n",
    "    #saver.save(session, save_as)\n",
    "    print('Saved model', save_as)\n",
    "\n",
    "    # learning rate update\n",
    "    if best_valid_loss is not None and np.exp(avg_valid_loss) > np.exp(best_valid_loss) - 1.0:\n",
    "        current_learning_rate = session.run(learning_rate)\n",
    "        current_learning_rate *= 0.5\n",
    "        \n",
    "        if current_learning_rate < 1.e-5:\n",
    "            break\n",
    "        \n",
    "        session.run(learning_rate.assign(current_learning_rate))\n",
    "        print('new learning rate is:', current_learning_rate)\n",
    "    else:\n",
    "        best_valid_loss = avg_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_state = session.run(initial_rnn_state)\n",
    "\n",
    "count = 0\n",
    "avg_loss = 0\n",
    "start_time = time.time()\n",
    "for x, y in test_reader.iter():\n",
    "    count += 1\n",
    "    t_loss, rnn_state = session.run([\n",
    "        loss,\n",
    "        final_rnn_state\n",
    "    ], {\n",
    "        input_ : x,\n",
    "        targets: y,\n",
    "        initial_rnn_state: rnn_state,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "\n",
    "    avg_loss += t_loss\n",
    "\n",
    "avg_loss /= count\n",
    "\n",
    "print(\"test loss = %6.8f, perplexity = %6.8f\" % (avg_loss, np.exp(avg_loss)))\n",
    "print(\"test samples:\", count*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
